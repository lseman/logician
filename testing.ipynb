{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8047fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created.\n",
      "Tool added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/back_home/baseline/foreblocks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 1242.23it/s, Materializing param=pooler.dense.weight]                              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-base-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI agent specialized in time series analysis and forecasting. I can help you with forecasting, anomaly detection, data exploration, and more. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from src import create_agent, ToolParameter\n",
    "\n",
    "agent = create_agent(\n",
    "    embedding_model=None,\n",
    "    config_overrides={\"rag_enabled\": False},\n",
    ")\n",
    "print(\"Agent created.\")\n",
    "def ping_tool(text: str) -> str:\n",
    "    return f\"PONG: {text}\"\n",
    "\n",
    "agent.add_tool(\n",
    "    name=\"ping\",\n",
    "    description=\"Simple echo ping.\",\n",
    "    function=ping_tool,\n",
    "    parameters=[ToolParameter(\"text\", \"string\", \"Text to echo\")],\n",
    ")\n",
    "print(\"Tool added.\")\n",
    "\n",
    "print(agent.chat(\"Say hello (no tools).\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104614f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! Feel free to return whenever you need help with time series analysis. Have a great day! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"Say bye (no tools).\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8500cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arxiv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# API SDKs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marxiv\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemanticscholar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticScholar  \u001b[38;5;66;03m# noqa: F401  (kept for future use)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhabanero\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Crossref\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arxiv'"
     ]
    }
   ],
   "source": [
    "# academic_agent.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "AcademicAgent â€” Literature search + RAG over papers (with debug output)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import inspect\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Any, Dict, List, Optional, Tuple, Iterable\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "\n",
    "# API SDKs\n",
    "import arxiv\n",
    "from semanticscholar import SemanticScholar  # noqa: F401  (kept for future use)\n",
    "from habanero import Crossref\n",
    "\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "    HAS_PYPDF = True\n",
    "except Exception:\n",
    "    HAS_PYPDF = False\n",
    "\n",
    "# Framework imports\n",
    "from core import Agent, ToolParameter\n",
    "\n",
    "\n",
    "from academic_agent import create_academic_agent\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Example Usage\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    agent = create_academic_agent()\n",
    "\n",
    "    # PHASE 1 â€” SEARCH (no LLM)\n",
    "    agent.search(\n",
    "        \"What are the latest transformer architectures for NLP?\",\n",
    "        per_source_limit=8,\n",
    "        order=\"s2,arxiv,crossref\",\n",
    "        open_access_only=False,\n",
    "    )\n",
    "\n",
    "    # agent.ingest_all(use_pdf=True, chunk_size=512, overlap=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80606840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Very small English stopword set, focused on question boilerplate\n",
    "_S2_STOPWORDS = {\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"whose\", \"where\", \"when\", \"why\", \"how\",\n",
    "    \"are\", \"is\", \"am\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"the\", \"a\", \"an\", \"of\", \"for\", \"to\", \"in\", \"on\", \"and\", \"or\", \"with\",\n",
    "    \"latest\", \"recent\", \"new\", \"newest\", \"current\", \"state\", \"art\",\n",
    "    \"paper\", \"papers\", \"article\", \"articles\",\n",
    "    \"about\", \"regarding\", \"related\", \"using\", \"use\", \"based\",\n",
    "    \"do\", \"does\", \"did\", \"can\", \"could\", \"should\", \"would\",\n",
    "}\n",
    "\n",
    "\n",
    "def _simplify_query(q: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristically simplify a natural-language question into a keyword-style\n",
    "    query that works better with Semantic Scholar's /paper/search.\n",
    "\n",
    "    Example:\n",
    "        \"What are the latest transformer architectures for NLP?\"\n",
    "        -> \"transformer architectures nlp\"\n",
    "    \"\"\"\n",
    "    if not q:\n",
    "        return q\n",
    "\n",
    "    # Lowercase and strip surrounding whitespace\n",
    "    q_clean = q.strip().lower()\n",
    "\n",
    "    # Remove trailing ?! and other punctuation at ends\n",
    "    q_clean = re.sub(r\"[?!\\.\\,;:\\s]+$\", \"\", q_clean)\n",
    "\n",
    "    # Tokenize: keep alphanumerics, drop everything else\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", q_clean)\n",
    "    if not tokens:\n",
    "        return q.strip()\n",
    "\n",
    "    # Remove stopwords if we have enough tokens\n",
    "    filtered = [t for t in tokens if t not in _S2_STOPWORDS]\n",
    "    # If everything was stripped, fall back to original tokens\n",
    "    if not filtered:\n",
    "        filtered = tokens\n",
    "\n",
    "    # Make a compact keyword query\n",
    "    simplified = \" \".join(filtered)\n",
    "\n",
    "    # Avoid returning something *longer* than the original by mistake\n",
    "    if len(simplified) > len(q.strip()):\n",
    "        return q.strip()\n",
    "\n",
    "    return simplified or q.strip()\n",
    "\n",
    "print(_simplify_query_for_s2(\"What are the latest transformer architectures for NLP?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ff7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "import httpx\n",
    "\n",
    "# Default fields from Semantic Scholar Graph API\n",
    "S2_DEFAULT_FIELDS = (\n",
    "    \"paperId,title,authors,venue,year,publicationDate,publicationTypes,\"\n",
    "    \"externalIds,url,openAccessPdf,abstract\"\n",
    ")\n",
    "\n",
    "\n",
    "def s2_search(\n",
    "    query: str,\n",
    "    limit: int = 10,\n",
    "    offset: int = 0,\n",
    "    open_access_only: bool = False,\n",
    "    fields: str = S2_DEFAULT_FIELDS,\n",
    "    base_url: str = \"https://api.semanticscholar.org/graph/v1\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Standalone Semantic Scholar Graph API search.\n",
    "    Works directly inside Jupyter, no CLI, no dependencies on your agent.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"total\": int,\n",
    "            \"offset\": int,\n",
    "            \"items\": [ list of paper dicts ],\n",
    "            \"rate_limited\": bool,\n",
    "            \"retry_after\": Optional[str],\n",
    "            \"raw\": dict\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare API key if available\n",
    "    api_key = os.getenv(\"S2_API_KEY\") or os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    headers = {\n",
    "        \"User-Agent\": \"StandaloneS2Search/1.0\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    if api_key:\n",
    "        headers[\"x-api-key\"] = api_key\n",
    "    else:\n",
    "        print(\"[WARN] No S2_API_KEY found â€” you may hit rate limits.\")\n",
    "\n",
    "    client = httpx.Client(headers=headers, timeout=20.0)\n",
    "\n",
    "    # Validate fields\n",
    "    fields_list = [f.strip() for f in fields.split(\",\") if f.strip()]\n",
    "    fields_param = \",\".join(fields_list) if fields_list else S2_DEFAULT_FIELDS\n",
    "\n",
    "    # Build request\n",
    "    limit = max(1, min(int(limit), 100))\n",
    "    offset = max(0, int(offset))\n",
    "\n",
    "    params: Dict[str, Any] = {\n",
    "        \"query\": query,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "        \"fields\": fields_param,\n",
    "    }\n",
    "\n",
    "    # Server-side open access filter\n",
    "    if open_access_only:\n",
    "        params[\"openAccessPdf\"] = \"\"\n",
    "\n",
    "    url = base_url.rstrip(\"/\") + \"/paper/search\"\n",
    "\n",
    "    # Execute request\n",
    "    try:\n",
    "        resp = client.get(url, params=params)\n",
    "        status = resp.status_code\n",
    "        print(f\"S2 HTTP {status}\")\n",
    "    except Exception as e:\n",
    "        print(\"Request failed:\", e)\n",
    "        return {\n",
    "            \"total\": None,\n",
    "            \"offset\": None,\n",
    "            \"items\": [],\n",
    "            \"rate_limited\": False,\n",
    "            \"retry_after\": None,\n",
    "            \"raw\": {\"error\": str(e)},\n",
    "        }\n",
    "\n",
    "    # Handle 429 explicitly\n",
    "    if resp.status_code == 429:\n",
    "        retry_after = resp.headers.get(\"Retry-After\")\n",
    "        print(f\"[RATE LIMITED] Retry-After={retry_after}\")\n",
    "        return {\n",
    "            \"total\": None,\n",
    "            \"offset\": None,\n",
    "            \"items\": [],\n",
    "            \"rate_limited\": True,\n",
    "            \"retry_after\": retry_after,\n",
    "            \"raw\": {\"status\": 429},\n",
    "        }\n",
    "\n",
    "    # Handle non-200-ish\n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Non-200 error from S2:\", e)\n",
    "        try:\n",
    "            raw = resp.json()\n",
    "        except Exception:\n",
    "            raw = {\"text\": resp.text}\n",
    "        return {\n",
    "            \"total\": None,\n",
    "            \"offset\": None,\n",
    "            \"items\": [],\n",
    "            \"rate_limited\": False,\n",
    "            \"retry_after\": None,\n",
    "            \"raw\": raw,\n",
    "        }\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        raw = resp.json()\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse error:\", e)\n",
    "        return {\n",
    "            \"total\": None,\n",
    "            \"offset\": None,\n",
    "            \"items\": [],\n",
    "            \"rate_limited\": False,\n",
    "            \"retry_after\": None,\n",
    "            \"raw\": {\"text\": resp.text},\n",
    "        }\n",
    "\n",
    "    # Interpret payload\n",
    "    if not isinstance(raw, dict):\n",
    "        print(\"Unexpected response format:\", type(raw))\n",
    "        return {\n",
    "            \"total\": None,\n",
    "            \"offset\": None,\n",
    "            \"items\": [],\n",
    "            \"rate_limited\": False,\n",
    "            \"retry_after\": None,\n",
    "            \"raw\": raw,\n",
    "        }\n",
    "\n",
    "    total = raw.get(\"total\")\n",
    "    offset_val = raw.get(\"offset\")\n",
    "    items = raw.get(\"data\")\n",
    "\n",
    "    # Some S2 deployments omit data when total==0\n",
    "    if items is None:\n",
    "        if isinstance(total, int) and total == 0:\n",
    "            print(\"S2 returned total=0 with no data field.\")\n",
    "            items = []\n",
    "        else:\n",
    "            print(\"[WARN] Missing 'data' field:\", raw)\n",
    "            items = []\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"offset\": offset_val,\n",
    "        \"items\": items,\n",
    "        \"rate_limited\": False,\n",
    "        \"retry_after\": None,\n",
    "        \"raw\": raw,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3766df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = s2_search(\"transformer architecture nlp\", limit=5)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5169d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nltk.download('punkt_tab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foreblocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
